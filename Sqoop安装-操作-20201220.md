﻿﻿﻿# Sqoop安装操作## 1. Sqoop安装解压Sqoop并重命名```tar -xvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C ../hadoopmv sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop-1.4.7```添加至 /etc/profile中```export SQOOP_HOME=/usr/hadoop/sqoop-1.4.7export PATH=$SQOOP_HOME/bin:$PATH```刷新环境配置```source /etc/profile```移动jdbc驱动到lib目录中```cp mysql-connector-java-5.1.49.jar /usr/hadoop/sqoop-1.4.7/lib/```查看sqoop版本```[root@master sqoop-1.4.7]# sqoop versionWarning: /usr/hadoop/sqoop-1.4.7/bin/../../hcatalog does not exist! HCatalog jobs will fail.Please set $HCAT_HOME to the root of your HCatalog installation.Warning: /usr/hadoop/sqoop-1.4.7/bin/../../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/hadoop/hadoop-2.10.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/hadoop/hbase-1.6.0/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]20/12/16 10:45:28 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7Sqoop 1.4.7git commit id 2328971411f57f0cb683dfb79d19d4d19d185dd8Compiled by maugli on Thu Dec 21 15:59:58 STD 2017```## 2. Sqoop参数export参数与import参数类似，请查看官方文档[](http://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html)### 2.1 Common arguments| Argument                   | Description                                                  || -------------------------- | ------------------------------------------------------------ || `--connect <jdbc-uri>`     | Specify JDBC connect string                                  || `--password <password>`    | Set authentication password                                  || `--username <username>`    | Set authentication username                                  || `--append`                 | Append data to an existing dataset in HDFS                   || `--columns <col,col,col…>` | Columns to import from table                                 || `--delete-target-dir`      | Delete the import target directory if it exists              || `-m,--num-mappers <n>`     | Use *n* map tasks to import in parallel                      || `--split-by <column-name>` | Column of the table used to split work units. Cannot be used with `--autoreset-to-one-mapper` option. || `--table <table-name>`     | Table to read                                                || `--target-dir <dir>`       | HDFS destination dir                                         || `--warehouse-dir <dir>`    | HDFS parent for table destination                            |### 2.2 Hive arguments| rgument                      | Description                                                  || ---------------------------- | ------------------------------------------------------------ || `--hive-home <dir>`          | Override `$HIVE_HOME`                                        || `--hive-import`              | Import tables into Hive (Uses Hive’s default delimiters if none are set.) || `--hive-overwrite`           | Overwrite existing data in the Hive table.                   || `--create-hive-table`        | If set, then the job will fail if the target hive            ||                              | table exists. By default this property is false.             || `--hive-table <table-name`   | Sets the table name to use when importing to Hive.           || `--hive-drop-import-delims`  | Drops *\n*, *\r*, and *\01* from string fields when importing to Hive. || `--hive-delims-replacement`  | Replace *\n*, *\r*, and *\01* from string fields with user defined string when importing to Hive. || `--hive-partition-key`       | Name of a hive field to partition are sharded on             || `--hive-partition-value <v>` | String-value that serves as partition key for this imported into hive in this job. |### 2.3 HBase argumentsIf the target table and column family do not exist, the Sqoop job will exit with an error. You should create the target table and column family before running an import. If you specify `--hbase-create-table`, Sqoop will create the target table and column family if they do not exist, using the default parameters from your HBase configuration.| Argument                     | Description                                                  || ---------------------------- | ------------------------------------------------------------ || `--column-family <family>`   | Sets the target column family for the import                 || `--hbase-create-table`       | If specified, create missing HBase tables                    || `--hbase-row-key <col>`      | Specifies which input column to use as the row key           ||                              | In case, if input table contains composite                   ||                              | key, then <col> must be in the form of a                     ||                              | comma-separated list of composite key                        ||                              | attributes                                                   || `--hbase-table <table-name>` | Specifies an HBase table to use as the target instead of HDFS |### 2.4 Export control arguments| Argument                   | Description                                                  || -------------------------- | ------------------------------------------------------------ || `--columns <col,col,col…>` | Columns to export to table                                   || `-m,--num-mappers <n>`     | Use *n* map tasks to export in parallel                      || `--table <table-name>`     | Table to populate                                            || `--export-dir <dir>`       | HDFS source path for the export                              || `--update-key <col-name>`  | Anchor column to use for updates. Use a comma separated list of columns if there are more than one column. || `--update-mode <mode>`     | Specify how updates are performed when new rows are found with non-matching keys in database. ||                            | Legal values for `mode` include `updateonly` (default) and `allowinsert`. |## 3. Sqoop操作### 3.1 获取MySQL所有数据库```sqoop list-databases \--connect jdbc:mysql://master:3306 \--username root \--password ylw@Y156```### 3.2 MySQL -> HDFS在mysql中新建test数据库，创建表users，使用sqoop导出数据到hdfs中```sqoop import \--connect jdbc:mysql://master:3306/hive?useSSL=false \--username root \--password ylw@Y156 \--table TBLS --m 1```sqoop默认导出位置为 user/用户名，查看users表信息```hadoop fs -cat /user/root/TBLS/*```### 3.3 MySQL -> Hive需要先将 hive-exec-xxx.jar 导入至 sqoop/lib 中，否则报错：*ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf*```sqoop import \--connect jdbc:mysql://master:3306/hive?useSSL=false \--username root \--password ylw@Y156 \--table COLUMNS_V2 \--hive-table COLUMNS_V10 \--fields-terminated-by ',' \--hive-import \--delete-target-dir```### 3.4 MySQL -> HBase```sqoop import \--connect jdbc:mysql://master:3306/hive?useSSL=false \--username root \--password ylw@Y156 \--table DBS \ --hbase-create-table \--hbase-table "DBS" \ --hbase-row-key "DB_ID" \--column-family "info" \--column 'DB_ID,DESC,DB_LOCATION_URI,NAME,OWNER_NAME,OWNER_TYPE'```会报错，怀疑HBase版本较高（2.3.3），Sqoop版本较低（1.4.7），双方版本不兼容```Exception in thread "main" java.lang.NoSuchMethodError: org.apache.hadoop.hbase.client.HBaseAdmin.<init>(Lorg/apache/hadoop/conf/Configuration;)V```可以降低版本，或将数据先导入至HDFS，然后使用BulkLoad导入至HBase### 3.5 HDFS、Hive -> MySQL查看HDFS中的数据dept001```[root@master lib]# hadoop fs -cat /hive/warehouse/dept001/*10,ACCOUNTING,NEW YORK20,RESEARCH,DALLAS30,SALES,CHICAGO40,OPERATIONS,BOSTON```创建表```CREATE TABLE dept(    id INT NOT NULL PRIMARY KEY,	dname VARCHAR(32),    loc VARCHAR(32));```导出数据至MySQL```sqoop export \--connect jdbc:mysql://master:3306/hive?useSSL=false \--username root \--password ylw@Y156 \--table dept \--export-dir /hive/warehouse/dept001 \--input-fields-terminated-by ','```追加导入，新增一行数据```hadoop fs -get /hive/warehouse/dept001/dept.csvvi dept.csvhadoop fs -put -f dept.csv /hive/warehouse/dept001/dept.csvhadoop fs -cat /hive/warehouse/dept001/*```导出数据至MySQL```sqoop export \--connect jdbc:mysql://master:3306/hive?useSSL=false \--username root \--password ylw@Y156 \--table dept \--export-dir /hive/warehouse/dept001 \--input-fields-terminated-by ',' \--update-key id \--update-mode allowinsert ```## Tips：http://sqoop.apache.org/docs/1.4.7/SqoopDevGuide.html推荐使用DataXhttps://github.com/alibaba/DataX